# A total of 16 32GB GPUs were used for training.

includes:
  - configs/s2ef/all/base.yml

# disable dataset so we explicitly set it when running the makefile
# dataset:
#   train:
#     format: lmdb
#     src: datasets/lmdb/alexandria_1_train.lmdb # Note: if the dataset is so large it won't fit into one lmdb file, point this to a directory containing multiple lmdb files
#     key_mapping:
#       y: energy
#       force: forces
#     # transforms:
#     #   normalizer:
#     #     energy:
#     #       mean: -0.7554450631141663
#     #       stdev: 2.887317180633545
#     #     forces:
#     #       mean: 0
#     #       stdev: 2.887317180633545
#   val:
#     src: datasets/lmdb/alexandria_1_val.lmdb

model:
  name: upgraded_escn
  num_layers: 6
  max_neighbors: 20
  cutoff: 6.0
  sphere_channels: 128
  hidden_channels: 256
  # sphere_channels: 160
  # hidden_channels: 384
  lmax_list: [3]
  mmax_list: [2]
  num_sphere_samples: 128
  distance_function: "gaussian"
  regress_forces: True
  use_pbc: True
  basis_width_scalar: 2.0
  otf_graph: True
  max_num_elements: 90

optim:
  batch_size: 12
  eval_batch_size: 12
  num_workers: 8 # do not increase too high. it'll cause gpu memeory pinning issues
  lr_initial: 0.0002
  optimizer: AdamW
  optimizer_params: {"amsgrad": True}
  eval_every: 500
  lr_gamma: 0.3
  lr_milestones: # epochs at which lr_initial <- lr_initial * lr_gamma
    - 218750
    - 281250
    - 343750
  warmup_steps: 100
  warmup_factor: 0.2
  max_epochs: 64
  # max_epochs: 2400000
  force_coefficient: 100
  energy_coefficient: 4
  clip_grad_norm: 5
  ema_decay: 0.999
  checkpoint_every: 5000

# optim:
#   batch_size:               12
#   eval_batch_size:          12
#   load_balancing:           atoms
#   num_workers:              8
#   lr_initial:               0.0002

#   optimizer:                AdamW
#   optimizer_params:
#     weight_decay:           0.001
#   scheduler:                LambdaLR
#   scheduler_params:
#     lambda_type:            cosine
#     warmup_factor:          0.2
#     warmup_epochs:          0.1
#     lr_min_factor:          0.01

#   max_epochs:               20
#   force_coefficient:        100
#   energy_coefficient:       4
#   clip_grad_norm:           20
#   ema_decay:                0.999

#   eval_every:               5000