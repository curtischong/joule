# A total of 16 32GB GPUs were used for training.

includes:
  - configs/s2ef/all/base.yml

dataset:
  train:
    format: lmdb
    src: datasets/lmdb/alexandria_1_train.lmdb # Note: if the dataset is so large it won't fit into one lmdb file, point this to a directory containing multiple lmdb files
    key_mapping:
      y: energy
      force: forces
    # transforms:
    #   normalizer:
    #     energy:
    #       mean: -0.7554450631141663
    #       stdev: 2.887317180633545
    #     forces:
    #       mean: 0
    #       stdev: 2.887317180633545
  val:
    src: datasets/lmdb/alexandria_1_val.lmdb

model:
  name: upgraded_escn
  # num_layers: 12
  num_layers: 4
  max_neighbors: 20
  cutoff: 8.0
  sphere_channels: 128
  hidden_channels: 256
  lmax_list: [6]
  mmax_list: [2]
  num_sphere_samples: 128
  distance_function: "gaussian"
  regress_forces: True
  use_pbc: True
  basis_width_scalar: 2.0
  otf_graph: True

optim:
  batch_size: 6
  eval_batch_size: 6
  num_workers: 8
  lr_initial: 0.0008
  optimizer: AdamW
  optimizer_params: {"amsgrad": True}
  eval_every: 5000
  lr_gamma: 0.3
  lr_milestones: # epochs at which lr_initial <- lr_initial * lr_gamma
    - 218750
    - 281250
    - 343750
  warmup_steps: 100
  warmup_factor: 0.2
  # max_epochs: 24
  max_epochs: 2400000
  force_coefficient: 100
  energy_coefficient: 4
  clip_grad_norm: 20
  ema_decay: 0.999
